[
  {
    "objectID": "khatami_analysis.html",
    "href": "khatami_analysis.html",
    "title": "Analyzing Prior Research",
    "section": "",
    "text": "Khatami, Mountrakis, and Stehman (2016) conducted a meta-analysis of studies on supervised pixel-based land-cover classification methods in remote sensing. Their focus was on examining the relative performance of different classification algorithms (e.g., Support Vector Machines (SVM), Decision Trees (DT), Neural Networks (NN)) and input data enhancements (e.g., texture, multi-angle imagery) used in these classification processes. Unlike, my study, which weighed studies based on their sample sizes, Khatami et al. used each paper as the unit of analysis. They did not incorporate the sample size of the original studies into their analysis. Instead, they used pairwise comparisons (Wilcoxon signed-rank test) based on the outcomes reported in the studies.\n\nDescriptive Statistics\nLike in my study, Khatami, Mountrakis, and Stehman (2016), found that overall accuracy is the most commonly reported metric in remote sensing classification studies, because of this they also used overall accuracy as the measure of classification performance in their meta-analysis.\nBecause of the way Khatami, Mountrakis, and Stehman (2016) recorded their data, I only use the model and publication year as study features. An important feature that I included in my study that was not in Khatami, Mountrakis, and Stehman (2016) is the proportion of the majority class. Khatami, Mountrakis, and Stehman (2016) discuss the issue of class imbalance but they were unable to correct for this because of the way that they did their analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\nBecause Khatami, Mountrakis, and Stehman (2016) didn’t use sample size, as an example here I set all the unknown sample sizes to 100.\n\nfull_dataset$sample_size &lt;-\n  ifelse(is.na(full_dataset$total), 100, full_dataset$total)\n\nSince overall accuracy is the number of correctly classified instances over the total number of instances. The number of events is\n\nfull_dataset$event &lt;- full_dataset$sample_size * full_dataset$OA_reported\n\nFrom the metafor package (Viechtbauer 2010), the escalc function, transforms the overall accuracy —here using the Freeman-Tukey double arcsine transformation— and calculates the appropriate variance for each observation.\n\nies.da &lt;- escalc(xi = event , ni = sample_size , \n                data = full_dataset,\n                measure = \"PFT\",  # FT double  arcsine  transformation\n                slab = paste(AuthorYear, \" Estimate \", esid)) \n\n\nMeta-analysis Results\nThe table presents the results of a meta-analysis conducted on two datasets (the dataset used in my thesis: “Nina, 2024” and Khatami, Mountrakis, and Stehman (2016)), as well as the combined data. The results show significant heterogeneity across all datasets, indicated by the very low \\(p_Q\\) values (&lt; 0.0001), meaning that the overall accuracy varies significantly from outcome to outcome. Variance at both Level 2 and Level 3 (\\(\\sigma^2_{\\text{level2}}\\) and \\(\\sigma^2_{\\text{level3}}\\)) is present in both datasets. Including study features (model group and publication year) does not reduce this variance in my dataset. However, in the Khatami, Mountrakis, and Stehman (2016) data, 55.5% of the variance at Level 2 is explained by the inclusion of these factors, though Level 3 variance remains poorly explained, with a negative \\(R^2_{\\text{level3}}\\) value (-5.6%).\n\n\n\n \n  \n    Dataset \n    $\\sigma^2_{\\text{level2}}$ \n    $\\sigma^2_{\\text{level3}}$ \n    $Q_E$ \n    df \n    $p_Q$ \n    $F$ \n    df \n    $p_F$ \n    $I^2_{\\text{level2}}$ \n    $I^2_{\\text{level3}}$ \n    $R^2_{\\text{level2}}$ \n    $R^2_{\\text{level3}}$ \n  \n \n\n  \n    Nina, 2024 \n    0.010 \n    0.016 \n    11862930 \n    80 \n    &lt;.0001 \n    1 \n    5 \n    0.397 \n    37.81 \n    62.19 \n    -0.4 \n    5.6 \n  \n  \n    Khatami, 2016 \n    0.003 \n    0.022 \n    7958 \n    722 \n    &lt;.0001 \n    27 \n    15 \n    &lt;.0001 \n    9.81 \n    80.91 \n    55.5 \n    -5.6 \n  \n  \n    Combined \n    0.004 \n    0.021 \n    11873557 \n    808 \n    &lt;.0001 \n    21 \n    15 \n    &lt;.0001 \n    16.58 \n    83.42 \n    35.7 \n    2.9 \n  \n\n\n\n\n\nKhatami, Mountrakis, and Stehman (2016) conducted a meta-analysis using articles that compared two or more classification algorithms applied to the same dataset, performing pairwise comparisons of their accuracy. They found that Support Vector Machines (SVM) consistently outperformed other classifiers. For instance, SVM performed better than Maximum Likelihood (ML) in 28 out of 30 studies, with a median improvement of 5%, and outperformed K-Nearest Neighbor (KNN) in 11 out of 13 studies. Random Forest (RF) also showed significant improvements over Decision Trees (DT), with a mean increase in accuracy of 4%. Additionally, KNN outperformed ML and DT in several comparisons, making it a viable alternative for certain classification tasks.\n\n\n\n \n\n\nTrasformed scale\nBack-transformed scale\n\n\n\nCI\n\n  \n    Feature \n    Estimate \n    SE \n    $p$ \n    Estimate$_{B-T}$ \n    LL \n    UL \n  \n \n\n  \n    Decision Trees \n    -0.01 \n    0.02 \n    0.591 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    Fuzzy \n    -0.02 \n    0.04 \n    0.649 \n    0.00 \n    0.01 \n    0.00 \n  \n  \n    Genetic Algorithm \n    0.10 \n    0.05 \n    0.053 \n    0.01 \n    NaN \n    0.04 \n  \n  \n    Index-Based \n    0.06 \n    0.05 \n    0.226 \n    0.00 \n    0.00 \n    0.02 \n  \n  \n    Immune System \n    0.06 \n    0.03 \n    0.04 \n    0.00 \n    0.00 \n    0.01 \n  \n  \n    K-Nearest Neighbor \n    0.01 \n    0.02 \n    0.61 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    Minimum Distance \n    -0.12 \n    0.02 \n    &lt;.0001 \n    0.01 \n    0.02 \n    0.01 \n  \n  \n    Maximum Likelihood \n    -0.01 \n    0.01 \n    0.36 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    Neural Networks \n    0.03 \n    0.02 \n    0.031 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    Parallelepiped \n    -0.27 \n    0.02 \n    &lt;.0001 \n    0.07 \n    0.10 \n    0.05 \n  \n  \n    Random Forest \n    0.03 \n    0.02 \n    0.197 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    Spectral Angle Mapper \n    -0.03 \n    0.03 \n    0.169 \n    0.00 \n    0.01 \n    0.00 \n  \n  \n    Subspace \n    0.03 \n    0.04 \n    0.434 \n    0.00 \n    0.00 \n    0.01 \n  \n  \n    Support Vector Machines \n    0.06 \n    0.01 \n    &lt;.0001 \n    0.00 \n    0.00 \n    0.01 \n  \n  \n    Publication.Year \n    0.00 \n    0.00 \n    0.546 \n    0.00 \n    0.00 \n    0.00 \n  \n\n\n\n\n\nThe comparison between the table results and Khatami, Mountrakis, and Stehman (2016) reveals consistent findings regarding the performance of several classification algorithms. Both analyses confirm that Support Vector Machines (SVM) consistently outperform other classifiers, with the table showing a significant positive effect (0.06, p=0.0001). Neural Networks (NN) also show positive performance in both studies, though slightly less effective than SVM. Random Forest (RF) performs well in Khatami et al. (with significant improvements over Decision Trees (DT)), but the table presents a non-significant effect for RF. KNN is another strong performer in Khatami, Mountrakis, and Stehman (2016), but its effect is not significant in the table (p=0.61). Meanwhile, Minimum Distance and Parallelepiped classifiers show poor performance in both analyses, with significant negative estimates in the table, aligning with Khatami et al.’s findings that these methods underperform. Thus, the table largely supports Khatami, Mountrakis, and Stehman (2016)’s conclusions, particularly regarding SVM’s dominance and the underperformance of simpler classifiers like ML and DT.\n\n\n\n\n\n\nReferences\n\nKhatami, Reza, Giorgos Mountrakis, and Stephen V. Stehman. 2016. “A Meta-Analysis of Remote Sensing Research on Supervised Pixel-Based Land-Cover Image Classification Processes: General Guidelines for Practitioners and Future Research.” Remote Sensing of Environment 177 (May): 89–100. https://doi.org/10.1016/j.rse.2016.02.028.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting Meta-Analyses in r with the Metafor Package” 36. https://doi.org/10.18637/jss.v036.i03.",
    "crumbs": [
      "Using Khatami's Dataset"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evaluating Machine Learning Models in Remote Sensing for Sustainable Development Goals",
    "section": "",
    "text": "In a world facing climate change, biodiversity loss, and urbanization, global efforts like the Sustainable Development Goals (SDGs) are crucial. Monitoring progress toward these goals is challenging, and to achieve accurate data on a large scale, we must look to innovative technology. Machine learning models combined with remote sensing provide a powerful way to track environmental and societal changes from space.\n\n\n\n\n\nThis post walks through a meta-analysis (a study that combines results from multiple studies) I conducted for my masters thesis, which focused on understanding how well machine learning models perform when applied to remote sensing data for the SDGs and which study features affect the performance. The Active Analysis Dashboard also gives you the opportunity to add data (see contribute data page). The analysis automatically updated using this data.\n\n\nMachine Learning Model Accuracy: On average, machine learning models in this field deliver an impressive 90% accuracy in classifying and predicting outcomes, such as land use, crop types, and forest cover, as seen across 20 studies and 86 results. It is important to note that this is a small sample size and only looked at results from published papers — this accuracy would almost certainly be lower if I also included unpublished work.\n\n\n\n\n\n\n\n\nFigure 1: Reported overall accuracy colour-coded by SDG goal. Individual outcomes shown as points and mean overall accuracy represented by triangles.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Map of researched locations\n\n\n\n\n\nOne key aspect of my analysis was multi-model inference, a technique used to compare multiple possible models to identify which study features most influence machine learning performance. In this study a total of 31,298 models were fit. By evaluating these combinations, the method determines which features consistently appear in the top-performing models, highlighting their importance.\n\n\n\n\n\n\nFigure 3: Model-averaged predictor importance plot\n\n\n\nFactors That Influence Performance: the majority class proportion i.e. the class imbalance in data (where one type of data is far more frequent then another) and the use of ancillary data (additional non-remote sensing data like information on ground temperature), were found to significantly affect model performance. Interestingly, the study found no major difference in performance between different types of machine learning models such as neural networks and tree-based models.\n\n\n\n\n\n\nFigure 4: Bubble plot showing the observed effect size, overall accuracy of the individual studies plotted against a the proportion of the majority class. The color of the points is unique to each study, with the lowest overall actuary from each study labeled with the first author and publication year.\n\n\n\nIt is not surprising that the overall accuracy is high when the majority class proportion is large because the model can perform well simply by predicting the majority class most of the time. Which leads to inflated accuracy values that do not necessarily reflect good performance across all classes.\nObserved vs. Predicted Accuracy: When using a model that include majority class proportion, ancillary data, and the use of remote sensing indices (the best fit model I have) the predicted accuracies tend to be overestimated.\n\n\n\n\n\n\nFigure 5: ADD\n\n\n\n\n\n\n\nThe normality assumption refers to the expectation that the distribution of the overall accuracy (effect size) in the studies follows a normal (bell-shaped) distribution. This assumption is important for the validity of certain statistical models.\nIn practice, the observed accuracy values tend to be skewed. To address this skewness and make the data more suitable for analysis, transformation techniques are applied. In my analysis, I used the Freeman-Tukey (FT) transformation. These transformations help stabilize the variance and make the distribution more symmetrical. The FT transformation, in particular, performed better in this case, although normality was still not perfectly achieved. In the following plot you can see the distributions of the effect size and their corresponding variance.\n\n// Initialize the data\ndf = transpose(my_data_ojs);\n\n\n\n\n\n\n\nviewof plotType = Inputs.select([\"Effect Size\", \"Variance\"], {label: \"Select Parameter\"});\n\n// Dropdown to select the transformation\nviewof transformation = Inputs.select([\n  \"Raw Proportion\", \n  \"Freeman-Tukey double arcsine transformed\", \n  \"Log transformed\", \n  \"Logit transformed\", \n  \"Arcsine square root transformed\"\n], {label: \"Select Transformation\"});\n\n\n\n\n// Create a reactive dataset based on the selected transformation and plot type\ntransformedData = df.map(row =&gt; {\n  if (plotType === \"Variance\") {\n    if (transformation === \"Log transformed\") {\n      return { ...row, value: row.PLN_vi };\n    } else if (transformation === \"Freeman-Tukey double arcsine transformed\") {\n      return { ...row, value: row.PFT_vi };\n    } else if (transformation === \"Logit transformed\") {\n      return { ...row, value: row.PLO_vi };\n    } else if (transformation === \"Arcsine square root transformed\") {\n      return { ...row, value: row.PAS_vi };\n    } else {\n      return { ...row, value: row.raw_vi }; // Original data (variance is not applicable here)\n    }\n  } else {\n    // Plot effect sizes (default behavior)\n    if (transformation === \"Log transformed\") {\n      return { ...row, value: row.PLN_yi };\n    } else if (transformation === \"Freeman-Tukey double arcsine transformed\") {\n      return { ...row, value: row.PFT_yi };\n    } else if (transformation === \"Logit transformed\") {\n      return { ...row, value: row.PLO_yi };\n    } else if (transformation === \"Arcsine square root transformed\") {\n      return { ...row, value: row.PAS_yi };\n    } else {\n      return { ...row, value: row.OA_reported }; // Original data\n    }\n  }\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Plot based on the selected transformation and plot type\nPlot.plot({\n  round: true,\n  color: {legend: true},\n  marks: [\n    Plot.rectY(transformedData, Plot.binX({y2: \"count\"}, {x: \"value\"})),\n    Plot.ruleY([0])\n  ], \n // y: {\n   // domain: [0, 45]  // Adjust y-axis based on your data\n//  }, \n  x: {\n    label: plotType === \"Variance\" ? \"Variance\" : \"Effect Size\"  // Label changes based on plot type\n  }\n});",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#lay-summary",
    "href": "index.html#lay-summary",
    "title": "Evaluating Machine Learning Models in Remote Sensing for Sustainable Development Goals",
    "section": "",
    "text": "In a world facing climate change, biodiversity loss, and urbanization, global efforts like the Sustainable Development Goals (SDGs) are crucial. Monitoring progress toward these goals is challenging, and to achieve accurate data on a large scale, we must look to innovative technology. Machine learning models combined with remote sensing provide a powerful way to track environmental and societal changes from space.\n\n\n\n\n\nThis post walks through a meta-analysis (a study that combines results from multiple studies) I conducted for my masters thesis, which focused on understanding how well machine learning models perform when applied to remote sensing data for the SDGs and which study features affect the performance. The Active Analysis Dashboard also gives you the opportunity to add data (see contribute data page). The analysis automatically updated using this data.\n\n\nMachine Learning Model Accuracy: On average, machine learning models in this field deliver an impressive 90% accuracy in classifying and predicting outcomes, such as land use, crop types, and forest cover, as seen across 20 studies and 86 results. It is important to note that this is a small sample size and only looked at results from published papers — this accuracy would almost certainly be lower if I also included unpublished work.\n\n\n\n\n\n\n\n\nFigure 1: Reported overall accuracy colour-coded by SDG goal. Individual outcomes shown as points and mean overall accuracy represented by triangles.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Map of researched locations\n\n\n\n\n\nOne key aspect of my analysis was multi-model inference, a technique used to compare multiple possible models to identify which study features most influence machine learning performance. In this study a total of 31,298 models were fit. By evaluating these combinations, the method determines which features consistently appear in the top-performing models, highlighting their importance.\n\n\n\n\n\n\nFigure 3: Model-averaged predictor importance plot\n\n\n\nFactors That Influence Performance: the majority class proportion i.e. the class imbalance in data (where one type of data is far more frequent then another) and the use of ancillary data (additional non-remote sensing data like information on ground temperature), were found to significantly affect model performance. Interestingly, the study found no major difference in performance between different types of machine learning models such as neural networks and tree-based models.\n\n\n\n\n\n\nFigure 4: Bubble plot showing the observed effect size, overall accuracy of the individual studies plotted against a the proportion of the majority class. The color of the points is unique to each study, with the lowest overall actuary from each study labeled with the first author and publication year.\n\n\n\nIt is not surprising that the overall accuracy is high when the majority class proportion is large because the model can perform well simply by predicting the majority class most of the time. Which leads to inflated accuracy values that do not necessarily reflect good performance across all classes.\nObserved vs. Predicted Accuracy: When using a model that include majority class proportion, ancillary data, and the use of remote sensing indices (the best fit model I have) the predicted accuracies tend to be overestimated.\n\n\n\n\n\n\nFigure 5: ADD\n\n\n\n\n\n\n\nThe normality assumption refers to the expectation that the distribution of the overall accuracy (effect size) in the studies follows a normal (bell-shaped) distribution. This assumption is important for the validity of certain statistical models.\nIn practice, the observed accuracy values tend to be skewed. To address this skewness and make the data more suitable for analysis, transformation techniques are applied. In my analysis, I used the Freeman-Tukey (FT) transformation. These transformations help stabilize the variance and make the distribution more symmetrical. The FT transformation, in particular, performed better in this case, although normality was still not perfectly achieved. In the following plot you can see the distributions of the effect size and their corresponding variance.\n\n// Initialize the data\ndf = transpose(my_data_ojs);\n\n\n\n\n\n\n\nviewof plotType = Inputs.select([\"Effect Size\", \"Variance\"], {label: \"Select Parameter\"});\n\n// Dropdown to select the transformation\nviewof transformation = Inputs.select([\n  \"Raw Proportion\", \n  \"Freeman-Tukey double arcsine transformed\", \n  \"Log transformed\", \n  \"Logit transformed\", \n  \"Arcsine square root transformed\"\n], {label: \"Select Transformation\"});\n\n\n\n\n// Create a reactive dataset based on the selected transformation and plot type\ntransformedData = df.map(row =&gt; {\n  if (plotType === \"Variance\") {\n    if (transformation === \"Log transformed\") {\n      return { ...row, value: row.PLN_vi };\n    } else if (transformation === \"Freeman-Tukey double arcsine transformed\") {\n      return { ...row, value: row.PFT_vi };\n    } else if (transformation === \"Logit transformed\") {\n      return { ...row, value: row.PLO_vi };\n    } else if (transformation === \"Arcsine square root transformed\") {\n      return { ...row, value: row.PAS_vi };\n    } else {\n      return { ...row, value: row.raw_vi }; // Original data (variance is not applicable here)\n    }\n  } else {\n    // Plot effect sizes (default behavior)\n    if (transformation === \"Log transformed\") {\n      return { ...row, value: row.PLN_yi };\n    } else if (transformation === \"Freeman-Tukey double arcsine transformed\") {\n      return { ...row, value: row.PFT_yi };\n    } else if (transformation === \"Logit transformed\") {\n      return { ...row, value: row.PLO_yi };\n    } else if (transformation === \"Arcsine square root transformed\") {\n      return { ...row, value: row.PAS_yi };\n    } else {\n      return { ...row, value: row.OA_reported }; // Original data\n    }\n  }\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Plot based on the selected transformation and plot type\nPlot.plot({\n  round: true,\n  color: {legend: true},\n  marks: [\n    Plot.rectY(transformedData, Plot.binX({y2: \"count\"}, {x: \"value\"})),\n    Plot.ruleY([0])\n  ], \n // y: {\n   // domain: [0, 45]  // Adjust y-axis based on your data\n//  }, \n  x: {\n    label: plotType === \"Variance\" ? \"Variance\" : \"Effect Size\"  // Label changes based on plot type\n  }\n});",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "codebook.html",
    "href": "codebook.html",
    "title": "Instructions",
    "section": "",
    "text": "Sadly the current set-up is a bit convoluted and geared towards GitHub users. For a introduction on using Git and GitHub through R see: Happy Git and GitHub for the useR.\nIf you have an GitHub account:\n\nFork the repository\nGo to the GitHub repository and click the “Fork” button. This creates a copy of the repository in your GitHub account.\nClone Your Fork\nIn your forked repository, click the green “Code” button, copy the URL, and clone it to your local machine using Git:\ngit clone https://github.com/your-username/your-forked-repo.git\nUpdate the CSV File\n\nIn your cloned repository, navigate to the CSV file (e.g., data/Contrib_metaData.csv).\nOpen it and add your data in the same format as the existing rows.\nSave the file.\n\nCommit and Push Your Changes\ngit add data/quotes.csv\ngit commit -m \"Added new data to the CSV\"\ngit push origin main\nSubmit a Pull Request\n\nGo back to your fork on GitHub and click “Contribute” &gt; “Open Pull Request.”\nSubmit your pull request, and I will review and merge your changes!",
    "crumbs": [
      "Contribute Data."
    ]
  },
  {
    "objectID": "codebook.html#how-to-add-to-the-dataset",
    "href": "codebook.html#how-to-add-to-the-dataset",
    "title": "Instructions",
    "section": "",
    "text": "Sadly the current set-up is a bit convoluted and geared towards GitHub users. For a introduction on using Git and GitHub through R see: Happy Git and GitHub for the useR.\nIf you have an GitHub account:\n\nFork the repository\nGo to the GitHub repository and click the “Fork” button. This creates a copy of the repository in your GitHub account.\nClone Your Fork\nIn your forked repository, click the green “Code” button, copy the URL, and clone it to your local machine using Git:\ngit clone https://github.com/your-username/your-forked-repo.git\nUpdate the CSV File\n\nIn your cloned repository, navigate to the CSV file (e.g., data/Contrib_metaData.csv).\nOpen it and add your data in the same format as the existing rows.\nSave the file.\n\nCommit and Push Your Changes\ngit add data/quotes.csv\ngit commit -m \"Added new data to the CSV\"\ngit push origin main\nSubmit a Pull Request\n\nGo back to your fork on GitHub and click “Contribute” &gt; “Open Pull Request.”\nSubmit your pull request, and I will review and merge your changes!",
    "crumbs": [
      "Contribute Data."
    ]
  },
  {
    "objectID": "codebook.html#codebook",
    "href": "codebook.html#codebook",
    "title": "Instructions",
    "section": "Codebook",
    "text": "Codebook\nAll variables that are in the dataset with the categories and explaintions\n\nClick to expand\n\n\n\n \n  \n    Feature \n    Variable_name \n    Definition \n    Ranges/Categories Adopted \n  \n \n\n  \n    contributor \n    dataset \n    Contrinutor ID \n     \n  \n  \n    DOI \n    NA \n    Paper ID \n     \n  \n  \n    First Author and publication year \n    AuthorYear \n    Name(s) of authors \n    First auther and publication year used as study label. \n  \n  \n    Location \n    location \n    Location of the data used (country level) \n     \n  \n  \n    Overall Accuracy* \n    OA_reported \n    Effect size of interest \n     \n  \n  \n    Sample Size* \n    sample_size \n    The sample size (i.e.: number of pixels, or objects) \n     \n  \n  \n    Publication Year \n    Publication_Year \n    Year of publication \n     \n  \n  \n    Classification Type \n    classification_type \n    Unit of analysis in the primary study \n    Object-level, Pixel-level, Unclear \n  \n  \n    Model Group* \n    model_group \n    Type of algorithm used. Any group that makes up less than 5 is regrouped as other analysis \n    Use abbreviations: Decision Tree (DT), Discriminant Analysis\n(DA), Fuzzy (FZ), Genetic Algorithm (GA), Immune System (IS), Index-Based (IB), K-Nearest Neighbor (KNN), Maximum Likelihood (ML), MinimumDistance (MD), Neural Network\n(NN), Parallelepiped (PP), Random Forest (RF), Spectral Angle Mapper (SAM), Subspace (SS), and Support Vector Machines (SVM), Ot \n  \n  \n    Ancillary Data \n    ancillary \n    Use of non-RS data in the model \n    Remote Sensing Only, Ancillary Data Included \n  \n  \n    Indices \n    indices \n    Use of indices to enhance analysis \n    Used, Not Used \n  \n  \n    Remote Sensing Type \n    RS_device_type \n    Category of remote sensing \n    Active, Passive, Combined, Not Reported \n  \n  \n    Device Group \n    RS_device_group \n    Specific device extracted, then grouped \n    Landsat, Sentinel, Other, Not Reported \n  \n  \n    Number of Spectral Bands \n    RS_spectral_bands_no \n    Number of spectral bands used \n    Count the number of bands or NA \n  \n  \n    Spectral Bands group* \n    no_band_group \n    Number of spectral bands is regrouped \n    Low:1-4 , Mid:5-20,high &gt;20,  Not Reported:NA \n  \n  \n    Spatial Resolution \n    RS_spatital_resolution_m \n    Spatial resolution in meters \n    eg: 30, &lt;1, NA \n  \n  \n    Confusion Matrix* \n    Confusion_matrix \n    Whether a confusion matrix was present \n    Reported, Not Reported \n  \n  \n    Majority-class Proportion* \n    fraction_majority_class \n    The proportion of the largest class \n     \n  \n  \n    Device \n    Rs_devices \n    Type of remote sensing device \n    Satellite, Aerial Photographic Images",
    "crumbs": [
      "Contribute Data."
    ]
  }
]